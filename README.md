# lambda-qwen
## Running a Qwen LLM Model in a Lambda Function with llama.cpp

Running an LLM model in a Lambda function isnâ€™t ideal, primarily due to the lack of GPU resources. This is an experiment to see if itâ€™s possible at all, using the `Qwen/Qwen2.5-0.5B` model.

### Prequisites

ğŸ”§ [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
ğŸ [Python](https://www.python.org/downloads/)
ğŸ› ï¸ [Terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)
ğŸ³ [Docker](https://www.docker.com/get-started/)
â˜ï¸  An AWS account
ğŸ–¥ï¸ [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), configured to connect to your AWS account

### Getting started

1. ğŸ“‚ Clone the repository

```bash
  git clone https://github.com/blackstack-ltd/lambda-qwen.git
```

2. ğŸ”§ Run the installer/deployer

```bash
  cd lambda-qwen/src
  ./build.sh
```

3. ğŸš€ Invoke the Lambda function in your AWS environment with a string input containing the prompt.

```bash
  aws lambda invoke --function-name lambda_qwen --cli-binary-format raw-in-base64-out --payload '"Once upon a time ..."' output.txt
  cat output.txt
```

âš ï¸  Note: This functions cold start time is more than a minute, mostly due to having to load the model.

### Test a different model

At the top of the `build.sh` deployment script, there is a variable called `MODEL`.
You can change this to any other Hugging Face model name and test its compatibility.

ğŸ› ï¸ Make sure you adapt the memory_size parameter in terraform/lambda.tf to suit the size of the model (make it twice the size of the model for good measure).

â³ When you first run a new, larger model, also set the timeout parameter to the maximum value (900 seconds) until you know how long the Lambda needs to load and execute the model.

âš ï¸  Note: More computationally expensive models may exceed Lambdaâ€™s maximum execution time of 15 minutes.
